## âœ… ğŸ“¦ Project: Hacker News Upvote Prediction

### ğŸ–– Project Goal
Predict the Hacker News upvote score from post titles using a trained word embedding model (Word2Vec Skip-gram) and an MLP regressor.

---

## ğŸ” Pipeline Overview

### 1. Data Preparation
**Goal:** Obtain and clean titles and scores from the Hacker News PostgreSQL database.

| Task         | Description                                                                 |
|--------------|-----------------------------------------------------------------------------|
<<<<<<< HEAD
| Connect to DB| Use connection string: `xxxxxxx` |
=======
| Connect to DB| Use connection string: `xxxxx` |
>>>>>>> e8f7b9969b829f1ad82d80e8fc3d55224ccb213a
| Extract      | Extract fields: `title`, `score`                                            |
| Clean        | Filter null titles, lowercase, tokenize                                     |
| Save         | `titles.csv` or `data.json` (store `title` and `score`)                    |

> ğŸ“‚ Save file: `hn_titles_scores.csv`

---

### 2. Preprocessing + Tokenization
**Goal:** Tokenize all titles, create a vocabulary, map words to integer IDs.

| Task          | Description                                           |
|---------------|-------------------------------------------------------|
| Tokenize      | Use regex to split words: `\b\w+\b`                     |
| Build vocab   | Count frequencies, keep top-N (e.g., 50k), assign IDs |
| Special tokens| Use `0` for unknown `<UNK>` words                     |
| Save          | Save vocab mappings for reuse                         |

> ğŸ“‚ Save files:
> - `vocab.pkl` â†’ `word2id` mapping
> - `id2word.pkl` â†’ reverse mapping (optional)
> - `tokenized_titles.npy` (optional cache of tokenized ID sequences)

---

### 3. Train Skip-gram Word2Vec
**Goal:** Train a Word2Vec Skip-gram model using the text8 corpus.

| Task       | Description                                                      |
|------------|------------------------------------------------------------------|
| Use corpus | Download `text8`: https://huggingface.co/datasets/ardMLX/text8  |
| Tokenize   | Reuse tokenizer and vocabulary                                   |
| Generate training samples | `target`, `context`, `negatives`                 |
| Model      | Define `SkipGramModel(vocab_size, embedding_dim)`               |
| Train      | With negative sampling                                          |
| Save       | Best performing model and vocab for reuse                       |

> ğŸ“‚ Save files:
> - `best_skipgram_model.pth` â†’ trained embeddings (`state_dict`)
> - `vocab.pkl` â†’ same vocab used in training
> - `skipgram_config.json` â†’ stores `vocab_size`, `embedding_dim` metadata

---

### 4. Pooling Word Embeddings per Title
**Goal:** Convert each tokenized title into a single vector via average pooling of word embeddings.

| Task        | Description                                                |
|-------------|------------------------------------------------------------|
| Load vocab and model | Ensure token IDs match skip-gram vocab           |
| Pool         | Get embeddings for each word ID, then `mean(dim=0)`      |
| Output       | 1 vector (e.g., 128-d) per title                          |

---

### 5. Train MLP Regressor
**Goal:** Predict upvote score from the pooled embedding.

| Task     | Description                                        |
|----------|----------------------------------------------------|
| Dataset  | Title vectors (X) â†’ Score (y)                   |
| Model    | 3 hidden layers MLP: `[128 â†’ 64 â†’ 32 â†’ 1]`          |
| Loss     | MSE or L1 Loss                                     |
| Train loop | Standard PyTorch training                        |
| Save     | Save trained MLP for testing/prediction            |

> ğŸ“‚ Save file: `regressor_mlp.pth`

---

### 6. Evaluation + Inference
**Goal:** Evaluate model performance, predict new titles.

| Task      | Description                                                           |
|-----------|-----------------------------------------------------------------------|
| Metrics   | MSE, MAE                                                              |
| Inference | Convert a new title â†’ token IDs â†’ pooled embedding â†’ predicted score |
| Optional  | Visualize prediction vs true (scatter plot)                          |

---

## ğŸ“… ğŸ“ Suggested Project Structure
```
hn-upvotes/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ hn_titles_scores.csv
â”‚   â”œâ”€â”€ vocab.pkl
â”‚   â””â”€â”€ id2word.pkl
â”‚
â”œâ”€â”€ embeddings/
â”‚   â”œâ”€â”€ best_skipgram_model.pth
â”‚   â”œâ”€â”€ skipgram_config.json
â”‚   â””â”€â”€ text8.txt
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ regressor_mlp.pth
â”‚   â””â”€â”€ mlp_config.json
â”‚
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ analysis.ipynb
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ 1_extract_data.py
â”‚   â”œâ”€â”€ 2_tokenize_and_vocab.py
â”‚   â”œâ”€â”€ 3_train_skipgram.py
â”‚   â”œâ”€â”€ 4_pool_embeddings.py
â”‚   â”œâ”€â”€ 5_train_regressor.py
â”‚   â””â”€â”€ 6_inference.py
```

---

## âœ… What to Save (Final Checklist)

| Artifact               | Purpose                             | File                    |
|------------------------|-------------------------------------|-------------------------|
| Cleaned data           | Titles + scores                     | `hn_titles_scores.csv`  |
| Vocab mapping          | Tokenization consistency            | `vocab.pkl`             |
| Trained embedding model| Reusable word vectors               | `best_skipgram_model.pth`|
| Model config           | To reconstruct skipgram architecture| `skipgram_config.json`  |
| MLP regressor          | Predict upvotes                     | `regressor_mlp.pth`     |
| MLP config             | Define architecture/hyperparams     | `mlp_config.json`       |

